# 复试八股文汇总

## （1） transformer

### transformer结构相关

介绍transformer的整体结构
transformer中的encoder和decoder分别有什么作用
介绍transformer的输入输出流程
encoder和decoder的输入数据有什么区别
encoder和decoder之间如何进行数据传输？
为什么要使用Add进行残差连接
teacher-forcing是什么?

### attention相关

attention的公式？
attention的时间复杂度？
为什么在softmax之前除以根号下d_k
为什么用softmax？
Q.dot(K)的数学含义是？
为什么用multi-head attention？
为什么要用masked attention， mask是如何实现的
采用multi-head attention是否增加了计算的时间复杂度

### transformer训练和推理相关

transformer训练和推理都是并行的吗？
encoder和decoder在训练和推理时的数据输入有什么区别
transformer的损失函数是什么？

### transformer优势分析

transformer相比RNN的优势是什么？
RNN为什么不需要位置编码?



### 我的答案：

### transformer结构相关

介绍transformer的整体结构
transformer中的encoder和decoder分别有什么作用
A：encoder是输入的信息压缩；decoder是生成
介绍transformer的输入输出流程
encoder和decoder的输入数据有什么区别
encoder和decoder之间如何进行数据传输？
为什么要使用Add进行残差连接
A：其一，它能有效缓解梯度消失问题，在深度网络反向传播时，为梯度提供捷径，避免其在多层传递中不断衰减，确保梯度可有效传到前面层，支持训练更深网络。其二，促进信息高效流动，让模型专注学习输入与输出间的残差，降低学习难度，利于捕捉序列长距离依赖。其三，能提升模型性能和泛化能力，使训练更稳定、收敛更快，还可学习更丰富特征，减少过拟合。此外，它还简化了模型训练，让参数更新更稳定，降低训练震荡，可使用更大学习率并减少对超参数调整的依赖。

### attention相关

attention的公式？
A: Attention矩阵 = softmax((Q@K.T) / 根号下d_k)@V
attention的时间复杂度？
A：O(n²d)
为什么在softmax之前除以根号下d_k
A: softmax是一种激活函数，值域在(0, 1)，向量长度是一个超参数，所以希望取消向量长度对值的影响；而且从数学角度上来看，根据方差分析，除以根号下d_k可以让矩阵内的值方差为1
为什么用softmax？
Q.dot(K)的数学含义是？
为什么用multi-head attention？
为什么要用masked attention， mask是如何实现的
采用multi-head attention是否增加了计算的时间复杂度

### transformer训练和推理相关

transformer训练和推理都是并行的吗？
encoder和decoder在训练和推理时的数据输入有什么区别
transformer的损失函数是什么？
A：交叉熵损失函数

### transformer优势分析

transformer相比RNN的优势是什么？
A：并行度高，无长度依赖
RNN为什么不需要位置编码?
A：已经设计到模型内了

